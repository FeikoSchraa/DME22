{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Recap\n",
    "\n",
    "The workflow we have been working with, consists of:\n",
    "\n",
    "1. load data\n",
    "2. visualise data \n",
    "3. prepare data\n",
    "4. test models (hyperparameter tuning)\n",
    "\n",
    "Let's walk through all of that really fast as a recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = sns.load_dataset(\"iris\")\n",
    "# df.head()\n",
    "data = sns.load_dataset(\"penguins\")\n",
    "df = data.drop([\"island\", \"sex\"], axis=1).dropna()\n",
    "y = df[\"species\"]\n",
    "X = df.drop(\"species\", axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df, x=\"bill_length_mm\", y=\"bill_depth_mm\", hue=\"species\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = df.melt(id_vars = 'species')\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.boxplot(data = p, x = 'variable', y='value', hue='species')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = df.drop('species', axis=1)\n",
    "y = df['species']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = pd.DataFrame(X_train, y_train).reset_index()\n",
    "p = p.melt(id_vars = 'species')\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.boxplot(data = p, x = 'variable', y='value', hue='species')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Pick hyperparameters and fit a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pick a lot of different classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And test them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = 5\n",
    "classifiers = [\n",
    "    ('svc-linear', SVC(kernel='linear')),\n",
    "    ('svc-kernel', SVC(C=1.0)),\n",
    "    ('random-forest', RandomForestClassifier(n_estimators=100)),\n",
    "    ('naive bayes', GaussianNB()),\n",
    "    ('gaussian', GaussianProcessClassifier()),\n",
    "    ('kNN', KNeighborsClassifier(n_neighbors=5)),\n",
    "    ('decision tree', DecisionTreeClassifier(criterion='gini'))\n",
    "]\n",
    "\n",
    "for i, (name, clf) in enumerate(classifiers):\n",
    "    clf.fit(X_train, y_train)\n",
    "    result = cross_val_score(clf, X_test, y_test, cv = cv, scoring='f1_macro')\n",
    "    \n",
    "    mu = np.mean(result)\n",
    "    stderr = np.std(result)/np.sqrt(cv)\n",
    "\n",
    "    plt.scatter(i, mu, label=name)\n",
    "    plt.errorbar(i, mu, yerr=stderr)\n",
    "    plt.legend(loc=3)\n",
    "\n",
    "plt.xticks(np.arange(len(classifiers)), [name[0] for name in classifiers], rotation=45);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='linear')\n",
    "clf.fit(X_train, y_train)\n",
    "yhat = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "cfm = confusion_matrix(y_test, yhat)\n",
    "ConfusionMatrixDisplay.from_estimator(clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = f1_score(y_test, yhat, average=\"macro\")\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Gridsearch & Pipelines\n",
    "\n",
    "While this is a basic approach, that can certainly give you results, there are some things that can be improved.\n",
    "\n",
    "- We just picked a model without changing the default hyperparameters. Hypertuning them can really improve a model\n",
    "- There are a lot of places that can give mistakes, especially if we make more complex models with multiple steps. We can make our approach more solid by specifying the order.\n",
    "- There are a lot of basic steps that we always need to repeat in the same order. This can be reduced into an efficient function.\n",
    "\n",
    "So, while we still keep these steps:\n",
    "\n",
    "1. explore data\n",
    "2. prepare data (eg rescaling, reshaping)\n",
    "3. pick hyperparameters and fit the model\n",
    "4. evaluate the model\n",
    "\n",
    "It is a good practice to bundle both step 2. and 3. into pipelines. Let's start with step 3, the picking of the best hyperparameters for a model.\n",
    "\n",
    "## 2.1.1 Basic hyperparameter tuning\n",
    "\n",
    "Hyperparameter tuning basically comes down to answering the question: what is the optimal combination of parameters for my problem? \n",
    "\n",
    "We already tried this with list comprehension, to quickly create a range of powers of 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While that worked, we have to multiply the amount of parameters to calculate the size of the searchspace. \n",
    "\n",
    "So, considering seven different values for two hyperparameters, we are searching $7*7=49$ combinations.\n",
    "\n",
    "## 2.1.2 Exhaustive gridsearch & heatmaps\n",
    "We will both use `GridSearchCV` to improve our hyperparameter tuning, and heatmaps to improve the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'C': [10**x for x in range(-4,3)],\n",
    "              'gamma': [10**x for x in range(-4,3)]}\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This already has a lot of advantages. We can easily expand the amount of parameters to check. We use a lot less code. You are protected against making all sorts of errors. And you use cross validation, which can be coded by hand too, but will take even more code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The question now is: was this a good gridsearch? There is a lot of room between a value C=10 and C=100.\n",
    "\n",
    "The best approach is to do itterations:\n",
    "\n",
    "Initially, you start with a scale that uses powers of 10 to quickly sweep a big area. Why not check the whole range to start with? Well, you have to consider the size of the space you are searching. Testing with a granularity of 0.0001 over the range 0.0001 up till 100 will take way too much time, especially if we have to multiply the range of every hyperparameter.\n",
    "\n",
    "So, start big, and zoom in. You can finetune the values by using heatmaps (at least, for the 2D case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.2 Random Gridsearch & successive halving\n",
    "\n",
    "If the searchspace becomes too large to search with a grid, you can take several approaches.\n",
    "\n",
    "Because our trainingset is very small, the overhead of the halving makes things worse, but for large datasets this can actually be a smart way to decrease time. To show this, let's make a new dummy dataset with 1000 examples.\n",
    "\n",
    "The first is to use a random grid search, where you can define distributions over which hyperparameters are randomly sampled. You can combine this with lists (e.g. for the kernels)\n",
    "\n",
    "First, a full gridsearch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_samples=1000, random_state=42)\n",
    "\n",
    "\n",
    "param_grid = {'C': [10**x for x in range(-4,3)],\n",
    "              'gamma': [10**x for x in range(-4,3)],\n",
    "              'kernel': ['rbf', 'poly']}\n",
    "tic = time()\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n",
    "grid_search.fit(X, y)\n",
    "time_full = time()-tic\n",
    "\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, a randomize search. Our initial searchspace had a size of $7*7*2=98$. We will now just sample 30 parameter settings, which is about 30%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import scipy.stats as stats\n",
    "param_dist = {'C': stats.expon(scale=100), 'gamma': stats.expon(scale=.01),\n",
    "  'kernel': ['rbf', 'poly']}\n",
    "\n",
    "tic = time()\n",
    "random_search = RandomizedSearchCV(SVC(), param_distributions=param_dist,\n",
    "                                   n_iter=30, random_state=42)\n",
    "random_search.fit(X, y)\n",
    "time_random = time()-tic\n",
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another, more efficient, approach is to use successive halving. From the documentation:\n",
    "\n",
    ">Successive halving (SH) is like a tournament among candidate parameter combinations. SH is an iterative selection process where all candidates (the parameter combinations) are evaluated with a small amount of resources at the first iteration. Only some of these candidates are selected for the next iteration, which will be allocated more resources. \n",
    "\n",
    "This approach is based on an interesting [paper](https://arxiv.org/abs/1603.06560)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a grid-version, that uses less training samples for the worse models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_halving_search_cv  # noqa\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "tic = time()          \n",
    "sh = HalvingGridSearchCV(SVC(), param_grid, \n",
    "\t\t\tcv=5,\n",
    "                        factor=2)\n",
    "sh.fit(X, y)\n",
    "time_halving = time()-tic\n",
    "sh.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There also is a random version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import HalvingRandomSearchCV\n",
    "tic = time()          \n",
    "sh = HalvingRandomSearchCV(SVC(), param_dist, \n",
    "\t\t\tcv=5,\n",
    "      factor=2,\n",
    "      random_state=42)\n",
    "sh.fit(X, y)\n",
    "time_randomhalving = time()-tic\n",
    "sh.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_full, time_random, time_halving, time_randomhalving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's obvious that a smart choice of searching the hyperparameter space can save you a lot of time. There is always a tradeoff with accuracy, however. It depends on the type of problem and the resources at hand (in terms of both hardware and time available) what is the best choice for problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Pipelines\n",
    "We already introduced pipelines. Let's take some more time to get to know them.\n",
    "\n",
    "Scikit-learn provides a lot of \"transformers\". These can do all sorts of things:\n",
    "\n",
    "- Clean the data\n",
    "- reduce the dimensions of the data\n",
    "- feature generation (e.g. with text data)\n",
    "\n",
    "Every transformer should have a `.fit`, `.transform` and `.fit_transform` method. This means that we could also create our own transformers, as long as they have these methods.\n",
    "\n",
    "We can combine these steps into a `Pipeline`.\n",
    "All estimators in a pipeline, except the last one, must be transformers (i.e. must have a transform method). The last estimator may be any type (transformer, classifier, etc.).\n",
    "\n",
    "\n",
    "## 2.2.1 Gridsearch with a pipeline\n",
    "The pipeline is a list of (key, value) pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([ \n",
    "\t(\"scaler\", StandardScaler()),\n",
    "\t(\"svc\", SVC())\n",
    "])\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will take care of things like the correct scaling on the trainset, without leaking information from the testset, and it is very easy to expand. We can add a lot of different steps into the pipeline without loosing track of the sequence.\n",
    "\n",
    "With this in place, we can use the GridSearch like before. Only thing we need to change, is we need to tell specify for which step in the pipeline the hyperparameters are intended. We do that by adding the name of the step in front of the hyperparameters, separated by two underscores, like this \"svc__C\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'svc__C': [10**x for x in range(-4,3)],\n",
    "              'svc__gamma': [10**x for x in range(-4,3)]}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.2 Preprocessing with a pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, we need to transform our data. Let's imagine we have days of the weeks. To feed this into a model, we will usually need to transform this into a one-hot-encoding, where every day of the week has it's own column, and every column simly gets a value of 1 if it is that day of the week (hence the name)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "X = [[\"mon\"], [\"tue\"], [\"wed\"], [\"thu\"]]\n",
    "ohe = OneHotEncoder()\n",
    "X_ = ohe.fit_transform(X)\n",
    "X_.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works as expected. Now let's make some imaginary data, where someone is selling icecream. The dataset has four days of observation, the temperature and the amount of icecreams sold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [\n",
    "\t[\"mon\", 20.4, 26], \n",
    "\t[\"tue\", 21.7, np.nan],\n",
    "\t[\"wed\", 25.2, 55],\n",
    "\t[\"thu\", 24.5, 48]\n",
    "]\n",
    "X = pd.DataFrame(X, columns=['day', 'temp', \"icecream\"])\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for one day, the amount of icecream sold is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the seller to have profit, he needs to sell more than 30 icecreams. The last two days he made profit, so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [0, 0, 1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing the data, we would need to do a lot of things.\n",
    "For example, we might want to impute the missing data. Let's say we want to take the median value for missing data.\n",
    "\n",
    "Also, we would want to scale all the numeric data, and translate the categories into a one-hot-encoding.\n",
    "\n",
    "Of course we could do this line by line, separarting and combining the data.\n",
    "\n",
    "However, we can also use pipelines.\n",
    "\n",
    "We start with specifying the types of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric = [\"temp\", \"icecream\"]\n",
    "categories = [\"day\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make two pipelines. One has the imputer and the scaler for the numeric data.\n",
    "The other (consisting of just one step) is the one hot encoder.\n",
    "\n",
    "We can now use the `Columntransformer` (which is a type of pipeline too) to combine the two transormers. Note that we use three values: a name, the transformer, and the columns to which we want to apply the transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric),\n",
    "        ('cat', categorical_transformer, categories)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets test that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, this is really nice. We have imputed the missing values, scaled everything, made a one-hot-encoding and stiched everything together!\n",
    "\n",
    "Now, we can simply combine this step into another pipeline, where we feed this to a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', SVC())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with this, we can start doing everything we would want.\n",
    "We can fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame([[\"tue\", 18.0, 14]], columns=['day', 'temp', \"icecream\"])\n",
    "clf.predict(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And hypertune. Note how we need to chain the names into a chain to specify the type of `strategy` for the `imputer`, inside the `num` step of the `preprocessor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'preprocessor__num__imputer__strategy': ['mean', 'median'],\n",
    "    'classifier__C': [0.1, 1.0, 10, 100],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=2)\n",
    "grid_search.fit(X, y)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('dme22-EYaUUFkp-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "a7860b014c314f311c8b672f44810aa9dc3f93fa03e0304fc46ef46d9eb93290"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
